---
title: "STAT243 - Problem Set 8"
author: "Cristobal Pais - cpaismz@berkeley.edu"
date: "December 1st, 2017"
header-includes:
   - \usepackage{multirow}
   - \usepackage{booktabs}
   - \usepackage{amsmath}
   - \usepackage{algorithm}
   - \usepackage[noend]{algpseudocode}
output:
  pdf_document: default
  html_document:
    highlight: pygments
---

\section*{Problem 1: Importance Sampling}

The main objective of this problem consists of comparing the usefulness of different distributions when performing an importance sampling estimation depending on the shape of the tail associated with each distribution. As we know from theory, the method needs a distribution with a heavy tail as the sampling density in order to work properly. Therefore, we will test this assumption by comparing the importance sampling when an exponential and Pareto distributions are used as sampling distributions. 

We want to estimate $\phi = \mathbf{E}(X)$ and $\phi = \mathbf{E}(X^2)$ with respect to a density $f$. 

First, we will compare both distributions (exponential and Pareto) in order to visualize their tails and conclude based on our knowledge of the distributions and relevant plots which one has a heavier tail and thus, fits better as the sampling distribution for the importance sampling approach. Then, we will empirically compare the results obtained when each distribution plays this fundamental role during the estimation of $\phi$.

\subsection*{a) Pareto vs Exponential distributions}
In order to determine which tail decay more quickly between both distributions, we simply plot a series of graphs where we can compare the different distribution functions for some combinations of their relevant parameters: (1) $\lambda$ rate in the case of the exponential and (2) $\alpha$ (location) and $\beta$ (shape) in the case of the Pareto distribution.

\begin{itemize}
  \item[1.] First, we define two auxiliary functions that will allow us to create simple plots for both distributions. The fist one will plot the density of an exponential distribution while the second one will address the Pareto distribution. A simple data frame is created with the values obtained from the densities after generating a large $x$ vector with values between $0$ to $20$.
  
  Both functions return the plot object.
\end{itemize}

```{r Functions, echo=TRUE, eval=TRUE, cache=FALSE}
plotExp <- function(Lambda){
  # Create the dataframe with the density to plot
  x <- seq(0, 20, length.out=1000)
  dat <- data.frame(x = x, px = dexp(x, rate = Lambda))
  
  # Create the plot using visual enhancements
  p <- ggplot(dat, aes(x = x,y = px)) + 
        theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
        theme(axis.line = element_line(size = 1, colour = "black"),
              panel.grid.major = element_line(colour = "#d3d3d3"), 
              panel.grid.minor = element_blank(),
              panel.border = element_blank(), 
              panel.background = element_blank()) +
        theme(legend.position="top") +
        geom_line(colour = "#ff1a1a", alpha = 0.8) +
        labs(x = "x value", y = "density", 
             title = paste("exp(Lambda =", Lambda,")"))
  
  # Return the plot object
  return(p)
}

plotPareto <- function(Alpha, Beta){
  # Create the DataFrame with the density to plot
  x <- seq(0, 20, length.out=1000)
  dat <- data.frame(x=x, px = dpareto(x, location = Alpha, shape = Beta))
  
  # Create the plot using visual enhancements
  p <- ggplot(dat, aes(x = x,y = px)) + 
        theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
        theme(axis.line = element_line(size = 1, colour = "black"),
              panel.grid.major = element_line(colour = "#d3d3d3"), 
              panel.grid.minor = element_blank(),
              panel.border = element_blank(), 
              panel.background = element_blank()) +
        theme(legend.position="none") +
        geom_line(colour = "#1a75ff", alpha = 0.8) +
        labs(x = "x value", y = "density", 
             title = paste("Pareto (Alpha =", 
                         Alpha, ", Beta =", Beta, ")"))
  
  # Return the plot object
  return(p)
}
```

\begin{itemize}
  \item[2.] Once the functions are defined, we declare an extra auxiliary function that will allow us to plot multiple plots in only one figure for simplicity of the comparisons. This $multiplot()$ function is available online in the $ggplot2$ package website and here we are including it as-is from the website.
\end{itemize}

```{r Multiplot, echo=TRUE, eval=TRUE, cache=TRUE}
# Multiple plot function
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


\begin{itemize}
  \item[3.] Then, four different plots (different $\lambda$ values) are generated for the exponential distribution by calling the auxiliary functions already defined: $plotExp()$ and $multiplot()$.
\end{itemize}

```{r Tails, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Exponential distribution plots (density)", fig.height=4, fig.width=10}
# Exponential distribution
# Supress warnings
options(warn = -1)

# Load libraries
library(ggplot2)

# Define the rate and create a DataFrame
Lambda = c(1, 10, 0.5, 0.1)

# Create a simple plot for visualizing the distribution
p1 <- plotExp(Lambda[1]) 
p2 <- plotExp(Lambda[2]) 
p3 <- plotExp(Lambda[3]) 
p4 <- plotExp(Lambda[4]) 

multiplot(p1, p2, p3, p4, cols=2)
```

\begin{itemize}
  \item[4.] Similarly, four different plots (different $\alpha$ and $\beta$ values) are generated for the Pareto distribution by calling the auxiliary functions already defined: $plotPareto()$ and $multiplot()$.
\end{itemize}

```{r Tails2, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Pareto distribution plots (density)", fig.height=4, fig.width=10}
# Supress warnings
options(warn = -1)

# Loading library
suppressMessages(library(EnvStats))

# Pareto distribution 
Alpha = c(1, 10, 0.5, 0.1)
Beta = c(1, 2, 5, 10)

# Create a simple plot for visualizing the distribution
p1 <- plotPareto(Alpha[1], Beta[1]) 
p2 <- plotPareto(Alpha[2], Beta[2]) 
p3 <- plotPareto(Alpha[3], Beta[3]) 
p4 <- plotPareto(Alpha[4], Beta[4]) 

multiplot(p1, p2, p3, p4, cols=2)
```

Looking at the plots, we can easily notice that the distribution with the heavier tail is the Pareto distribution as we expected since we know (from our background and classes) that \textit{heavy-tailed distributions are probability distributions whose tails are not exponentially bounded: that is, they have heavier tails than the exponential distribution}. In this case, we know that the Pareto distribution is a subexponential distribution and thus, one of the classic heavy-tailed distributions.

Therefore, we will expect better estimation results using it as the sampling distribution for the importance sampling method. As we know from the lecture notes: \textit{things can be badly behaved if we sample from a density with lighter tails than the density of interest.}

\newpage

\subsection*{b) IS Estimation: Pareto distribution as sampling density}
In this case, we have that the function $f$ is an exponential distribution with $\lambda = 1$, shifted by two units to the right, such that we have:
\begin{eqnarray}
  f(x) = \lambda e^{-\lambda (x - 2)} = e^{2-x}
\end{eqnarray}

On the other hand, we will use a Pareto distribution with parameters $\alpha = 2$ and $\beta = 3$ as the $g$ function in the importance sampling method. Hence, we have:
\begin{eqnarray}
  g(x) = \dfrac{\beta \alpha^{\beta}}{x^{\beta + 1}} = \dfrac{24}{x^{4}}
\end{eqnarray}

Finally, we will use $m = 10000$ in order to estimate $\phi = \mathbf{E}(X)$ and $\phi = \mathbf{E}(X^{2})$, remembering that $\mathbf{VAR}(\hat{\phi}) \propto \mathbf{VAR}(h(X)f(X)/g(X))$. Histograms of $h(X)f(X)/g(X)$ and $f(X)/g(X)$ will be generated in order to identify wheter the variance $\mathbf{VAR}(\hat{\phi})$ is large. Extreme weights will be analyzed in order to check their impact on the estimation of $\hat{\mu}$.

\begin{itemize}
  \item[1.] First, we can easily define $w(X) = f(X)/g(X) = \dfrac{24e^{2-x}}{x^{4}}$ and we will drawn $m = 10000$ $x_i$ values from our sampling distribution $g(X) = Pareto(\alpha, \beta) = 24/x^{4}$ where $x \in ]\alpha, +\infty[$. Therefore, we perform the following operations:
\end{itemize}

```{r IS1, echo=TRUE, eval=TRUE, cache=TRUE}
# Perform the sampling operation
set.seed(0)
m = 10000
Alpha = 2
Beta = 3
x = rpareto(n = m, location = Alpha, shape = Beta)

# Define the f(x) function
fx <- function(x){
  return(exp(2 - x))
}

# Define the g(x) function
gx <- function(x){
  return(24 / (x ** 4))
}

# Determine w(X)
wx = (fx(x)/gx(x))
```

\begin{itemize}
  \item[2.] Now, since we have the weights $w(X)$, we can easily compute the mean value of the final expression in order to estimate the $\phi = \mathbf{E}(X)$ and $\phi = \mathbf{E}(X^{2})$ values.
\end{itemize}

```{r IS1_2, echo=TRUE, eval=TRUE, cache=TRUE}
# E(X) estimation
Ex = mean(x * wx)
print(paste0("Estimation of phi = E(x) = ", Ex))

# By real formula
ParetoMean = (Beta * Alpha) / (Beta - 1)
print(paste0("E(x) = ", ParetoMean))

# E(X^2) estimation
Ex2 = mean((x ** 2) * wx)
print(paste0("Estimation of phi = E(x^2) = ", Ex2))

# Variance estimation
# By formula VAR(X) = E[X^2] - E[X]^2
Varx = Ex2 - (Ex ** 2)
print(paste0("Estimation of VAR(phi) = ", Varx))

# By direct implementation
Varx_2 = var(x * wx)
print(paste0("Estimation of VAR(X * w(X)) = ", Varx_2))

# By real formula
ParetoVar = (Beta * (Alpha ** 2)) / (((Beta - 1) ** 2) * (Beta - 2)) 
print(paste0("VAR(X) = ", ParetoVar))
```

From the previous results, we can notice that the estimation of the mean value is pretty accurate (almost identical to the real mean of the distribution). On the other hand, it is possible to see that we are able to reduce the variance of the sampling distribution thanks to the importance sampling implementation with a heavy tailed distribution such as with the Pareto distribution we are using in this case. Therefore, the main objective of the approach is satisfied. 

\begin{itemize}
  \item[3.] Histograms for $h(X)f(X)/g(X)$ and the weights $w(X) = f(X)/g(X)$ are generated in order to get an idea wheter the $\mathbf{VAR}(\phi)$ is large.
\end{itemize}

```{r IS1_3, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="$h(X)w(X)$ histogram using shifted exp() distribution as sampling function", fig.height=4, fig.width=10}
# Create histograms
# h(X)f(X)/g(X)
qplot(x*wx, geom="histogram", 
      xlab = "h(X)f(X)/g(X)", 
      ylab = "frequency", 
      fill=I("#ff1a1a"), 
      col=I("black"),
      alpha = 0.8) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
    theme(axis.line = element_line(size = 1, colour = "black"),
          panel.grid.major = element_line(colour = "#d3d3d3"), 
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), 
          panel.background = element_blank()) +
    theme(legend.position="none") +
    labs(title = "Histogram for h(x)f(x)/g(x)")   
```

Looking at the previous histogram we can notice that several values are concentrated around the value of $2$ for the function under study. On the other hand, the maximum values are approximately $6.5$ units.

```{r IS1_4, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Weights $w(X)$ histogram using Pareto distribution as sampling function", fig.height=4, fig.width=10}
# Create histograms
# w(X) = f(X)/g(X)
qplot(wx, geom="histogram", 
      xlab = "w(X) = f(X)/g(X)", 
      ylab = "frequency", 
      fill=I("#1a75ff"), 
      col=I("black"),
      alpha = 0.8)+
    theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
    theme(axis.line = element_line(size = 1, colour = "black"),
          panel.grid.major = element_line(colour = "#d3d3d3"), 
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), 
          panel.background = element_blank()) +
    theme(legend.position="none") +
    labs(title = "Histogram for w(X) = f(X)/g(X)")   
```

Based on the previous plot containing the values of the weights, we can clearly see that there are not extreme weights that would have a very strong influence on $\hat{\mu}$ since the largest value is about `r round(max(wx), 3)`. Therefore, the estimation of the expected value of $X$ is very accurate.



\subsection*{c) IS Estimation: Exponential distribution as sampling density}
Repeating the same procedure as before while changing the role of $g$ and $f$, we have:

\begin{itemize}
  \item[1.] First, we can easily define $w(X) = f(X)/g(X) = \dfrac{x^{4}}{24e^{2-x}}$ and we will drawn $m = 10000$ $x_i$ values from our sampling distribution $g(X) = exp(\lambda)_{shifted} = e^{2-x}$ where $x \in ]2, +\infty[$. Therefore, we perform the following operations:
\end{itemize}

```{r IS2, echo=TRUE, eval=TRUE, cache=TRUE}
# Perform the sampling operation
set.seed(0)
m = 10000
Lambda = 1
x = rexp(n = m, rate = Lambda) + 2

# Define the f(x) function
fx <- function(x){
  return(24 / (x ** 4))
}

# Define the g(x) function
gx <- function(x){
  return(exp(2 - x))
}

# Determine w(X)
wx = (fx(x)/gx(x))
```

\begin{itemize}
  \item[2.] Now, since we have the weights $w(X)$, we can easily compute the mean value of the final expression in order to estimate the $\phi = \mathbf{E}(X)$ and $\phi = \mathbf{E}(X^{2})$ values.
\end{itemize}

```{r IS2_2, echo=TRUE, eval=TRUE, cache=TRUE}
# E(X) estimation
Ex = mean(x * wx)
print(paste0("Estimation of phi = E(x) = ", Ex))

# By real formula (solve integral from shifted mean)
ExpMean = 3
print(paste0("E(x) = ", ExpMean))

# E(X^2) estimation
Ex2 = mean((x ** 2) * wx)
print(paste0("Estimation of phi = E(x^2) = ", Ex2))

# Variance estimation
# By formula VAR(X) = E[X^2] - E[X]^2
Varx = Ex2 - (Ex ** 2)
print(paste0("Estimation of VAR(phi) = ", Varx))

# By direct implementation
Varx_2 = var(x * wx)
print(paste0("Estimation of VAR(X * w(X)) = ", Varx_2))

# By real formula (solving the integrals)
ExpVar = 1
print(paste0("VAR(X) = ", ExpVar))
```

From the previous results we can easily check that the mean value estimation is very accurate but in this case, the variance estimation surpass the true value of the distribution variance due to the fact that the exponential distribution is not as good as the Pareto distribution for applying the importance sampling methodology: it does not satisfy the assumption of being a heavy tail distribution. Therefore, we are not able to reduce the variance of the sampling.

\begin{itemize}
  \item[3.] Histograms for $h(X)f(X)/g(X)$ and the weights $w(X) = f(X)/g(X)$ are generated in order to get an idea wheter the $\mathbf{VAR}(\phi)$ is large.
\end{itemize}

```{r IS2_3, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="$h(X)w(X)$ histogram using shifted exp() distribution as sampling function", fig.height=4, fig.width=10}
# Create histograms
# h(X)f(X)/g(X)
hist1 <- qplot(x*wx, geom="histogram", 
      xlab = "h(X)f(X)/g(X)", 
      ylab = "frequency", 
      fill=I("#ff1a1a"), 
      col=I("black"),
      alpha = 0.8) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
    theme(axis.line = element_line(size = 1, colour = "black"),
          panel.grid.major = element_line(colour = "#d3d3d3"), 
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), 
          panel.background = element_blank()) +
    theme(legend.position="none") +
    labs(title = "Histogram for h(x)f(x)/g(x)")   

print(hist1)
```

Looking at the previous histogram we can notice that several values are concentrated around the value of $2-3$ for the function under study. On the other hand, the maximum values are approximate `r round(max(x*wx), 3)` units.

```{r IS2_4, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Weights $w(X)$ histogram using shifted exp() distribution as sampling function", fig.height=4, fig.width=10}
# Create histograms
# w(X) = f(X)/g(X)
qplot(wx, geom="histogram", 
      xlab = "w(X) = f(X)/g(X)", 
      ylab = "frequency", 
      fill=I("#1a75ff"), 
      col=I("black"),
      alpha = 0.8,
      xlim = c(0,15))+
    theme(axis.text.x = element_text(angle = 0, hjust = 1)) +
    theme(axis.line = element_line(size = 1, colour = "black"),
          panel.grid.major = element_line(colour = "#d3d3d3"), 
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), 
          panel.background = element_blank()) +
    theme(legend.position="none") +
    labs(title = "Histogram for w(X) = f(X)/g(X)")   
```

Based on the previous plot containing the values of the weights, we can clearly see that there are extreme weights that would have a very strong influence on $\hat{\mu}$ since the largest value is about `r round(max(wx), 3)`. Therefore, we can expect that the value of $\hat{\mu}$ will not be significantly impacted by these extreme weights since their frequency is very low in comparison to the rest of the variables, but the variance of the distribution will be significantly impacted by these extreme weights, as we already saw when comparing it to the variance of the original distribution (larger).



\newpage





\section*{Problem 2: Helical valley function}
In this problem, we want to study the so called $helical$ $valley$ function included in the $ps8.R$ file. The main idea consists of analyzing it behavior using visual tools such as $contour$ lines for ploting it in a two-dimensional fashion and then, after understanding the basic patterns of the function, try to optimize it and find its minimum by a series of different functions included in R such as $optim()$, $optimx()$, and $nlm()$. 

Finally, a series of five different starting points will be tested in order to check and explore the possibility of reaching multiple local minima.

\begin{itemize}
  \item[1.] We start by loading the file containing the function under study:
\end{itemize}

```{r LoadHel, echo=TRUE, eval=TRUE, cache=TRUE}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Load the function
source("ps8.R")

# Testing the function (f)
x = c(1,10,4)
f(x)
```

Now, we start analyzing the function.

\subsection*{a) Initial analysis: contours of the function}
In order to be able to generate beautiful and explicit contour plots of the function, we define an auxiliary function called $ContourPlot()$ that will fix the value of the third component of the vector $X$ (given as an argument of the function) in order to generate a slice of the three-dimensional function. The main logic of the function is as follows:

\begin{itemize}
  \item[i)] A grid for the components $X1$ and $X2$ is generated in order to be able to generate a surface. These vectors are manipulated in order to be able to create a DataFrame containing all the components (each row is a vector) needed to pass as arguments of the helical function. 
  \item[ii)] Using this DataFrame, a simple $apply()$ call using the helical function as the main input will allow us to evaluate each row/vector with the function, obtaining a new vector of $z$ values.
  \item[iii)] This vector is transformed into a matrix (surface format) in order to be able to use the classic contour functions from both the base and $ggplot$ packages.
  \item[iv)] Depending on the flag passed to the $color$ argument (boolean), a simple black and white or a colorful contour plot will be generated.
\end{itemize}

Therefore, the $ContourPlot()$ function is as follows:

```{r ContourFunction, echo=TRUE, eval=TRUE, cache=TRUE}
ContourPlot <- function(X3=0, color=TRUE){ 
  # Load the relevant function
  source("ps8.R")
  
  # Create the grid based on simple vectors
  X1 = seq(-300,300,1) 
  X2 = seq(-300,300,1)
  X11 = rep(X1, each=length(X2))
  X22 = rep(X2, length(X1))
  
  # Fix the third variable value
  X3 = rep(X3,length(X1)*length(X2))
  
  # Generate a DataFrame
  Dat = data.frame("x1" = X11, "x2" = X22, "x3" = X3)
  
  # Apply the helical function and transform output to a matrix (surface)
  Z = apply(Dat, 1, function(x) f(x))
  Z = matrix(Z,length(X1),length(X2))
  
  # Plot the contour
  if (color == FALSE) {
    fc <- contour(X1,X2,Z)
  }
  
  # Fancy plot
  if (color == TRUE){
    fc <- filled.contour(x = X1,
                       y = X2,
                       z = Z,
                       color.palette = colorRampPalette(c("yellow", "red")),
                       xlab = "X1",
                       ylab = "X2",
                       main = paste0("Helical contour X3 = ", X3[1]),
                       key.title = title(main = "Value", cex.main = 1))
  }
  
  # Return the plot object
  return(fc)
}
```

Thus, we only need to call the function using different values for the fixed $X3$ component of the vectors in order to obtain a series of different (and beautiful) contour plots:

```{r Helplot, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3 = 0$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Call the function for different values of X3
fc0 <- ContourPlot()
```

```{r Helplot2, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3= 1$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc1 <- ContourPlot(10)
```

```{r Helplot3, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3= 10$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc2 <- ContourPlot(100)
```

```{r Helplot4, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3= 1e3$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc3 <- ContourPlot(1000)
```

```{r Helplot5, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3= 1e4$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc4 <- ContourPlot(10000)
```

```{r Helplot6, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3 = 1e5$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc5 <- ContourPlot(100000)
```

```{r Helplot7, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3 = 1e6$ value)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc6 <- ContourPlot(1000000)
```

```{r Helplot8, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3 = -1e4$ value, no color example)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc7 <- ContourPlot(-1000, FALSE)
```

```{r Helplot9, echo=TRUE, eval=TRUE, cache=FALSE, fig.align="center", fig.cap="Different contour plots from the Helical function (fixed $X3 = -1e6$ value, no color example)", fig.height=6, fig.width=7}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Loading library
library("ggplot2")

# Generate the contour plot
fc8 <- ContourPlot(-100000, FALSE)
```


\newpage






\subsection*{b) Finding the minimum: $optim()$, $optimx()$, and $nlm()$ functions}
In this section, we would like to study the function in terms of its potential local minima points as well as try to find the global (if exists) minimum value of the helical function. In order to perform this task, we will define a series of auxiliary functions that will act as wrappers of the already studied $optim()$, $optimx()$, and $nlm()$ functions in R. The main idea and strategy will be the following:

\begin{itemize}
  \item[i)] Several starting points $X_0$ will be tested in order to find the minimum of the function. Thanks to this approach, we will be able to detect potential (multiple) local minima.
  \item[ii)] After each optimization with the $optim()$ and $nlm()$ functions, we will take the final solution as a new starting point, checking if the convergence of the algorithms was not enough, solving the minimization problem in an iterative way such that we can clearly identify which points are local minimums (if they exist). This procedure is avoided with the $optimx()$ function in order to contrast the iterative results with a one-optimization (classic) approach. Important is to note that the $optimx()$ function gives us more information about the type of solution reached by reporting the status of the KKT multipliers.
  \item[iii)] Different optimization methods such as BFGS and Nelder-Mead will be tested for comparison purposes.
\end{itemize}

Thus, we proceed as follows:
\begin{itemize}
  \item[1.] We first define our $OptF()$ auxiliary function that will take a starting point $x_0$ given by the user and will optimize the helical function. Once the new point and new objective value are obtained, we check if the current solution is an optimal point or a local minimum by re-optimizing the function from that point. We continue with this fashion until the solution converges (no improvement is obtained) to a certain value.  
\end{itemize}

```{r OptimF, echo=TRUE, eval=TRUE, cache=TRUE}
OptF <- function(X0 = c(0, 0, 0)){
  # Load relevant functions
  source("ps8.R")
  
  # Initial Objective Value
  InitialObj <- f(X0)
  
  # List of methods available
  Methods <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent")
  
  # Perform the optimization 
  Optimization <- optim(X0, f, method = Methods, control = (maxit = 1e6))
  
  # Initial Optimization summary
  NewObjValue <- Optimization$value
  NewPoint <- Optimization$par
  
  # Print results
  print("------- Optimization Results -------")
  print("Initial point:")
  print(X0)
  print(paste0("Minimum value obtained: ", NewObjValue))
  print("Optimal Point obtained:")
  print(NewPoint)
  cat("\n")

  # Update the initial objective function
  InitialObj = NewObjValue
  
  # Re-optimization loop: breaks if no improvement is attained
  while (TRUE) {
    # Checking if the algorithm got stuck in a local minimum: re-optimize
    print("------- Re-optimization from new point -------")
    Optimization <- optim(NewPoint, f, method = Methods)
    print("Initial point:")
    print(NewPoint)
    print(paste0("Minimum value obtained: ", Optimization$value))
    print("Optimal Point obtained:")
    print(Optimization$par)

    # Update the solution
    NewObjValue = Optimization$value
    NewPoint = Optimization$par
    
    # Check if no improvement occurs
    if (NewObjValue >= InitialObj){
      break
    }
    
    # Else, go to next iteration starting from the new point (update obj function)
    else {
      InitialObj = NewObjValue
      cat("\n")
    }
    
  } 
}
```

\begin{itemize}
  \item[2.] Thus, we test a series of different starting points $x_0$ in order to study the behavior of the algorithms when different $x_0$ values are provided. 
\end{itemize}

```{r Optim, echo=TRUE, eval=TRUE, cache=TRUE}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Initial points 
X0 = c(0, 0, 0)
X0_2 = c(1, 0, 0)
X0_3 = c(100, 100, 100)
X0_4 = c(1, 0, -1)
X0_5 = c(1, -1, 0)
X0_6 = c(0, 1, 0)
X0_7 = c(0, 0, 1)
X0_8 = c(1, 1, 1)
X0_9 = c(-1, -1, -1)
X0_10 = c(0, -1, 0)
X0_11 = c(1e8,-1e8,100)
InitialP = list(X0, X0_2, X0_3, X0_4, X0_5,
                X0_6, X0_7, X0_8, X0_9, X0_10,
                X0_11)

# Call the function
for (x0 in InitialP) {
  OptF(x0)
  cat("\n\n")
}
```

Based on the results, we can clearly see that some points such as $(1, -6.202e-10 -1.477e-09)$ reach a very low (close to zero) objective function value without reaching the optimal solution of the optimization problem, and thus, the algorithm got stuck in that (those) point(s) without further improvements in the objective function. However, we can notice that all solutions besides the one associated with the last starting point (very high magnitude of its components) tend to reach points very similar to the $(1,0,0)$. The last starting point provided is a particular case, acting as an outlier in terms of the magnitude of its components and the final objective value reached with some methods such as $Nelder-Mead$ is clearly not optimal (diverges), however, with other methods such as BFGS we can see that the solution tends to be the $(1,0,0)$ vector.

Thus, we can conclude that the optimization of the objective function is not particularlt very sensitive with respect to the given starting point provided and the method applied, noticing that some points are not good starting points for this particular function if we want to find the global optimum value in a few iterations. On the other hand, we can notice that several starting points reached a value that is not a exactly a global minimum after the first optimization, and hence, a series of iterative re-optimization steps are performed in order to reach a better solution. Comparing the previous results, we can clearly see that the best solution obtained at this point consists of $x^{*} = (1, 0, 0)$, using $x_0 = (1, 0, 0)$ as a starting point - notice that the algorithm does not improve this value - where the objective function is equal to $0$.

\begin{itemize}
  \item[3.] In order to check and compare the previous results with other optimization methods, we define a wrapper function $OptX()$ for calling the $optimx()$ function, an improved version of the original $optim()$ function. In this case, we perform a simple one-step call of the optimization methods in order to be able to study the results obtained by just one call of the function instead of using an iterative approach (like in the previous section). Therefore, we construct a DataFrame containing all the results from the optimization methods and we display the most relevant ones: optimal point, the optimal value found, and KKT condition check.
  
  Thus, the simple function is as follows (traditional optimization methods are implemented, we included the full list as a comment):
\end{itemize}

```{r Optimx, echo=TRUE, eval=TRUE, cache=FALSE}
# Function for optimizing from a starting point X0 using optimx function
OptX <- function(X0 = c(0, 0, 0)){
  # Supress warnings
  options(warn = -1)
  
  # Load relevant functions
  source("ps8.R")

  # Possible methods (selection of the most traditional ones)
  # Other options: "spg", "ucminf", "newuoa", "bobyqa", "nmkb", 
  #                "hjkb", "Rcgmin", or "Rvmmin"
  Methods <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "nlm", "nlminb")
  
  # Optimize
  print("----------------- Optimization results -----------------")
  print("Initial point:")
  print(X0)
  R <- data.frame(optimx(X0, f, method = Methods, itnmax=NULL))
  print(R[c(1:4, 9, 10)])
}
```

\begin{itemize}
  \item[4.] After defining the function, we are ready to load the corresponding package and compare the solutions obtained when using different starting points for the optimization methods.
\end{itemize}

```{r Optimx2, echo=TRUE, eval=TRUE, cache=FALSE}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Load libraries
library("optimx")

# Initial point 
X0 = c(0, 0, 0)
X0_2 = c(1, 0, 0)
X0_3 = c(100, 100, 100)
X0_4 = c(1, 0, -1)
X0_5 = c(1, -1, 0)
X0_6 = c(0, 1, 0)
X0_7 = c(0, 0, 1)
X0_8 = c(1, 1, 1)
X0_9 = c(-1, -1, -1)
X0_10 = c(0, -1, 0)
X0_11 = c(1e8,-1e8,100)
InitialP = list(X0, X0_2, X0_3, X0_4, X0_5,
                X0_6, X0_7, X0_8, X0_9, X0_10,
                X0_11)

# Call the function
for (x0 in InitialP) {
  OptX(x0)
  cat("\n\n")
}
```

Based on the results, we can clearly see that again, we have a great convergence of the solutions toward the vector $(1,0,0)$ where the value of the objective function is $0$. Looking at the helical function defined in $ps8.R$, we can clearly see that the final objective function consists of a summation of quadratic terms and thus, all of them are $\geq 0$. Therefore, it is possible to conclude that the optimal (global minimum) objective value of the function under study is $0$, reached at the point $(1,0,0)$, indicating us that the tested optimization methods tend to have a great convergence toward the global optimal solution of the function.

\begin{itemize}
  \item[5.] For completeness, we test the $nlm()$ function using the same starting points as before, in order to check its convergence towards the optimal value of the function. In order to do this, we define a new auxiliary variable $OptNLM()$ as a wrapper of the original function:
\end{itemize}

```{r nlmF, echo=TRUE, eval=TRUE, cache=TRUE}
# Define the auxiliary function
OptNLM <- function(X0 = c(0, 0, 0)){
  # Supress warnings
  options(warn = -1)
  
  # Load relevant functions
  source("ps8.R")
  
  # Perform the optimization 
  NLMOpt <- nlm(f, X0)
  
  # Initial Optimization summary
  NewObjValue <- NLMOpt$minimum
  NewPoint <- NLMOpt$estimate
  
  # Print results
  print("------- Optimization Results -------")
  print("Initial point:")
  print(X0)
  print(paste0("Minimum value obtained: ", NewObjValue))
  print("Optimal Point obtained:")
  print(NewPoint)
}
```

\begin{itemize}
  \item[6.] Finally, as before, we call the function in an iterative fashion for solving the optimization problem with different starting points:
\end{itemize}

```{r nlm, echo=TRUE, eval=TRUE, cache=TRUE}
# Set working directory
setwd("C:/Users/chile/Desktop/Stats243/HW/HW8/")

# Load libraries
library("optimx")

# Initial point 
X0 = c(0, 0, 0)
X0_2 = c(1, 0, 0)
X0_3 = c(100, 100, 100)
X0_4 = c(1, 0, -1)
X0_5 = c(1, -1, 0)
X0_6 = c(0, 1, 0)
X0_7 = c(0, 0, 1)
X0_8 = c(1, 1, 1)
X0_9 = c(-1, -1, -1)
X0_10 = c(0, -1, 0)
X0_11 = c(1e8,-1e8,100)
InitialP = list(X0, X0_2, X0_3, X0_4, X0_5,
                X0_6, X0_7, X0_8, X0_9, X0_10,
                X0_11)

# Call the function
for (x0 in InitialP) {
  OptNLM(x0)
  cat("\n\n")
}
```


By looking at the results, we can easily check that the results are consistent and coherent with the ones obtained by our previous implementations, where the optimization methods tend to converge to the optimal point $(1,0,0)$ from any of the tested starting points with different convergence rates (with a special exception with the origin, where the $nlm()$ function is not able to improve the original solution). Therefore, we have analyzed and studied the proposed helical function both using visual and optimization approaches.


\newpage






\section*{Problem 3: censored regression problem}
In this problem, we want to consider a simple linear censored regression problem with the functional form of:
\begin{eqnarray}
  Y_i \rightarrow N(\beta_0 + \beta_1 x_i , \sigma) \\
  Y_i = \beta_0 + \beta_1 x_i + \epsilon_i  \; \; \epsilon \rightarrow N(0,\sigma)
\end{eqnarray}

We will assume that we have an iid sample, but that for any observation with $Y > \tau$ we know that $Y$ exceeded the threshold and not its actual value. Following the example of the problem statement, we have that $c$ of the $n$ observations will be censored (stochastically) depending on how many of them exceed the fixed value $\tau$.

For the rest of the problem, we define $m = n-c$ with $c$ the number of censored values. Without loss of generality, we sort the values of the original $y$ vector such that the first $m$ components are the uncensored/observed values while the $m+1,...,n$ components consist of the censored values. Thus, we will denote the censored values by $z = (z_{m+1}, ..., z_{n})$ and by $y = (y_1, ..., y_m)$ the observed components.

Therefore, for simplicity, we can define:
\begin{eqnarray}
y_{i}^{*} = \begin{cases}
				y_i & i \leq m\\
				z_i & i > m
			\end{cases}
\end{eqnarray}

The main objective of this problem is to propose, design, and implement a $EM$ algorithm in order to estimate the three main parameters of the model: $\theta = (\beta_0, \beta_1, \sigma)$, while taking the complete data to be available in addition to the censored observations (actual values).

\subsection*{a) EM algorithm}

In order to develop our EM algorithm, we need to follow a series of steps that will allow us to formulate all the needed expressions needed for calculating the complete log-likelihood of the sampled data, the expected log-likelihood using all observations, and find the optimal values (maximization) of the $theta$ parameters per iteration of the algorithm.

\subsubsection*{Step 1: Complete-data log-likelihood}
The first step consists of calculating the complete-data log-likelihood. In this case, we do not take into account the fact that the $z_i \; i \in \lbrace m+1,...,n \rbrace$ values are censored.  First, we calculate the complete-data likelihood as follows:

\begin{eqnarray}
	L^{c}(\theta / y^{*}) &=& \prod_{i = 1}^{n} f(y_{i}^{*}) \\
	&=& \prod_{i = 1}^{m} f(y_{i}) \prod_{j = m+1}^{n} f(z_{j}) \\
	&=& \prod_{i = 1}^{m} \phi \left( \dfrac{y_i - x_{i}^{'} \beta}{\sigma} \right) \sigma^{-1} \prod_{j = m+1}^{n} \phi \left( \dfrac{\tau - x_{j}^{'} \beta}{\sigma} \right) \sigma^{-1} \\
	&=& \sigma^{-n} \prod_{i = 1}^{m} \phi \left( \dfrac{y_i - x_{i}^{'} \beta}{\sigma} \right)  \prod_{j = m+1}^{n} \phi \left( \dfrac{\tau - x_{j}^{'} \beta}{\sigma} \right)  \\
	&=& \sigma^{-n} \prod_{i = i}^{m} \dfrac{1}{\sqrt{2 \pi}}  e^{\dfrac{  -{(y_i - x_{i}^{'} \beta)}^{2}    }{2 \sigma^{2}}} \prod_{j = m+1}^{n} \dfrac{1}{\sqrt{2 \pi}}  e^{\dfrac{  -{(z_j - x_{j}^{'} \beta)}^{2}    }{2 \sigma^{2}}} \\
	&=& \sigma^{-n} \dfrac{1}{{(2 \pi)}^{n/2}} \prod_{i = i}^{m}  e^{\dfrac{  -{(y_i - x_{i}^{'} \beta)}^{2}    }{2 \sigma^{2}}} \prod_{j = m+1}^{n}  e^{\dfrac{  -{(z_j - x_{j}^{'} \beta)}^{2}    }{2 \sigma^{2}}}
\end{eqnarray}

Notice that we are using the standard density function for an $N(0,1)$ by standardizing the original distribution. In addition, we expanded the terms for simplicity when calculating the complete-data log-likelihood function. 

Now, we apply the logarithmic function to the previous expression as follows:
\begin{eqnarray}
	\log(L^c(\theta / y^*)) &=& - \underbrace{\dfrac{n}{2} \log(2 \pi)}_{C} - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \sum_{i = 1}^{m} {(y_i - x_{i}^{'} \beta)}^{2} - \dfrac{1}{2 \sigma^{2}} \sum_{j = m+1}^{n} {(z_j - x_{j}^{'} \beta)}^{2} \\
	&=& C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \left( \sum_{i = 1}^{m} {(y_i - x_{i}^{'} \beta)}^{2} + \sum_{j = m+1}^{n} {(z_j - x_{j}^{'} \beta)}^{2}   \right) \\
    &=&	C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \left( \sum_{i = 1}^{n} {(y_{i}^{*} - x_{i}^{'} \beta)}^{2} \right) \\
\end{eqnarray}

Looking at the previous expression, we can clearly see that is very similar to a classic linear regression model with the main difference that in this case, we have a set of censored values $z_j$ for which we do not know their real values, only that they satisfy $z_j > \tau \; j \in \lbrace m+1,...,n \rbrace$.

Now, we are ready to go to the next step and starting with the relevant calculations for the EM algorithm.

\subsubsection*{Step 2: Expected complete-data log-likelihood}
In order to use an EM algorithm approach, we need to calculate the expected value of the previous log-likelihood function, given that we have a vector of estimated parameters $\theta_t$ for the iteration $t$ of the algorithm and a vector $x$ of observations. Therefore, we want to find:
\begin{eqnarray}
	Q(\theta / \theta_t) &=& \mathbf{E}[\log(L^c(\theta / y^*)) | x, \theta_t]
\end{eqnarray}

This is known as the E-step in the EM algorithm. Notice that we are interested in calculating the expectation over the missing data in a closed form. In addition, we have to be extremely careful to separate the values of $theta$ (that we want to optimize) from the values of $theta_t$ (already estimated).

In order to calculate this expression, we proceed as follows:
\begin{eqnarray}
	Q(\theta / \theta_t) &=& \mathbf{E}[\log(L^c(\theta / y^*)) | x, \theta_t] \\
						&=& \mathbf{E} \left[ C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \left( \sum_{i = 1}^{m} {(y_i - x_{i}^{'} \beta)}^{2} + \sum_{j = m+1}^{n} {(z_j - x_{j}^{'} \beta)}^{2}   \right) | x, \theta_t \right]  \\
						&=& C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}}  \sum_{i = 1}^{m} {(y_i - x_{i}^{'} \beta)}^{2}   - \dfrac{1}{2 \sigma^{2}} \sum_{j = m+1}^{n} \underbrace{\mathbf{E} \left[  {(z_j - x_{j}^{'} \beta)}^{2} | x, \theta_t \right]}_{A}
\end{eqnarray}

Now, we can focus on the $A$ expression since we need to calculate the expected value of the censored components of the data, given that we know the $x$ vector, the $theta_t$ vector, and that $zi$ follows a truncated normal distribution by $\tau$.

In order to calculate this expression, we expand its components and we will use the known results and properties from both the expectation and the truncated normal distribution: 
\begin{eqnarray}
	\mathbf{E} \left[  {(z_j - x_{j}^{'} \beta)}^{2} | x, \theta_t \right] &=& \mathbf{E} \left[  z_{j}^{2} - 2 z_{j} x_{j}^{'} \beta + {(x_{j}^{'} \beta)}^{2} | x, \theta_t \right]  \\
	&=& \underbrace{\mathbf{E} \left[  z_{j}^{2} | x, \theta_t  \right]}_{B} - 2 \underbrace{\mathbf{E} \left[ z_{j} | x, \theta_t  \right]}_{C} x_{j}^{'} \beta + {(x_{j}^{'} \beta)}^{2} 
\end{eqnarray}

Now, we solve $B$:
\begin{eqnarray}
	\mathbf{E} \left[ z_{j}^{2} | x, \theta_t \right] &=& \mathbf{E} \left[ z_{j}^{2} | z_j > \tau, x, \theta_t \right] \\
	&=& \mathbf{Var} \left[ z_j | z_j > \tau  \right] + {(\mathbf{E} \left[ z_j | z_j > \tau  \right])}^{2} \\
	&=& \sigma_{t}^2 (1 + \tau_{t,j}^{*} \rho(\tau_{t,j}^{*}) - {\rho(\tau_{t,j}^{*})}^{2}) + {(\mu_{t,j} + \sigma_t \rho(\tau_{t,j}^{*}))}^{2}
\end{eqnarray}

And for $C$:
\begin{eqnarray}
	\mathbf{E} \left[ z_{j} | x, \theta_t \right] &=& \mathbf{E} \left[ z_{j} | z_j > \tau, x, \theta_t \right] \\
	&=& \mathbf{E} \left[  z_{j} | z_j > \tau \right] \\
	&=& \mu_{t,j} + \sigma_{t} \rho(\tau_{t,j}^{*}) 
\end{eqnarray}

where $\mu_{t,j} = \beta_{0}^{t} + \beta_{1}^{t} x_j$, $\tau_{t,j}^{*} = \dfrac{(\tau - \mu_{t,j})}{\sigma_t}$, and $\rho(\tau_{t,j}^{*}) = \dfrac{ \phi(\tau_{t,j}^{*})}{\left(1- \Phi(\tau_{t,j}^{*}   ) \right)}$

Having these values and re-arranging the terms by noting the fact that we have an analog expression to the classic quadratic difference term, we can finally obtain the expression for $Q(\theta / \theta_t)$:
\begin{eqnarray}
	Q(\theta / \theta_t) &=& C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \left( \sum_{i = 1}^{m} {(y_i - x_{i}^{'}\beta)}^{2} + \sum_{j = m+1}^{n} \left( {(\mathbf{E} [z_j] - x_j^{'}\beta)}^{2} + \mathbf{Var}[z_j]\right) \right) 	
\end{eqnarray}

Looking at the previous expression, we can clearly see that we have: (1) the same expressions as for a classic regression for the observed data, (2) a similar expression for the censored data where the values are replaced by the conditional expectation (given that $z_j > \tau$ of the variable $z_j$ (omitted in the expected value and variance of the previous equation for simplicity), and (3) an extra term including the variance of the censored observations.

Thus, we can easily express (as mentioned in the problem statement) the $Q$ function as a regression term plus the variance of the censored elements as follows:
\begin{eqnarray}
	Q(\theta / \theta_t) &=& C - n \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \left( \sum_{i = 1}^{n} {(\delta_i - x_{i}^{'}\beta)}^{2} + \sum_{j=m+1}^{n} \mathbf{Var}[z_j]\right) 	
\end{eqnarray} 

where $\delta_i = \begin{cases}
				y_i & i \leq m\\
				\mathbf{E}[z_i] & i > m
				\end{cases}$


\subsubsection*{Step 3: Optimal parameters expressions}
Finally, we need to calculate the expressions for calculating the optimal parameters $\theta = (\beta_0, \beta_1, \sigma$) such that the previous function is maximized. Clearly, the optimal $\hat{\beta}$ values can be easily obtained by performing a simple linear regression using adapted vectors $Y = (y_{obs}, y_{cen}) = (y,\mathbf{E}[z])$ and $X = (x_{obs}, x_{cen}) = x$ (assuming w.l.o.g. that the values are sorted). Therefore, we are simply solving a least squares problem in order to obtain both coefficients (check appendix for the specific expressions obtained by direct derivation of the $Q$ function).

On the other hand, we need to explicitly solve the maximization problem for $\sigma$ by taking the derivative of the $Q$ function with respect to $\sigma$ (or $\sigma^{2}$ if wanted). Thus, we obtain the following expression:
\begin{eqnarray}
	\dfrac{\partial Q(\theta / \theta_t)}{\partial \sigma} &=& - \dfrac{n}{\sigma} + \dfrac{1}{\sigma^{3}} \left( \sum_{i=1}^{m} {(y_i - x_{i}^{'}\beta)}^{2} + \sum_{j=m+1}^{n} {(z_j - x_{j}^{'}\beta)}^{2}  + \sum_{j=m+1}^{n} \mathbf{Var}[z_j] \right) \\
	&=& 0
\end{eqnarray}

Solving for $\sigma$ we get:
\begin{eqnarray}
	{\sigma^2}^{*} &=& \dfrac{1}{n}  \left( \sum_{i=1}^{m} {(y_i - x_{i}^{'}\beta)}^{2} + \sum_{j=m+1}^{n} {(z_j - x_{j}^{'}\beta)}^{2}  + \sum_{j=m+1}^{n} \mathbf{Var}[z_j] \right)
\end{eqnarray}

Therefore, at this point, we have all the expressions needed for developing our EM algorithm.



\subsubsection*{Algorithm}
The main logic of the algorithm is very simple: starting from a particular point for the optimization $\theta_0$, a series of iterations where the $E$ and the $M$ steps will be performed, obtaining a new vector $\theta_t$ during iteration $t$ such that the value of the $Q$ function is maximized (and thus, our log-likelihood function). The algorithm will iterate until a convergence threshold is satisfied like a difference between consecutive values of the objective function or the estimated parameter values or by a maximum number of possible iterations.

The proposed algorithm and its pseudo-code are the followings:

\begin{itemize}
	\item[0)] Global parameters are specified: Maximum number of iterations ($MaxIters$), convergence tolerance ($\epsilon$), empty list for the $\theta_t$ results, and a convergence boolean flag ($Convergence$) for checking the final status of the algorithm.
	\item[1)] We start our $EM$ algorithm by generating/separating the data depending if the observation is censored or not. In the case that the data is generated/simulated, we define a certain threshold $\tau$ for censoring it. 
	\item[2)] Once the data is separated by $Y = (y, z)$ and $X = (x_{obs}, x_{cen})$, we compute an initial (reasonable) starting point. In order to perform this step, a simple linear regression will be developed with the observed data such that the coefficients $\beta_0,\beta_1$ are initialized with these estimations and also, computing the standard deviation of the observed sample will be the selected value for initializing the $\sigma$ parameter. Hence, we initialize the algorithm with $\theta_0$ with the values described before.
	\item[3)] Once the initialization is done, the main loop of the algorithm begins. Using vectorized operations in R, all the relevant parameters needed for the calculations of the expected values, variances, and optimal $\theta_t$ parameters with $t$ the current iteration will be computed: $\mu_{t,j}$, $\tau_{t,j}^{*}$, $\rho(\tau_{t,j}^{*})$, $\mathbf{E}[z_j]$, $\hat{{\beta^{t}}^{*}}$, $\sigma_{t}^{*}$, and the $Log-likelihood$ function for convergence purposes.
	\item[4)] Breaking conditions will be checked after every iteration. A convergence checking based on the tolerance parameter will be performed by comparing the value of the objective function between two consecutive iterations. If the absolute change in its value is less than the specified tolerance, the algorithm breaks. In addition, the algorithm breaks if the maximum number of iterations is reached before achieving the convergence (base on the $\epsilon$ value) to a particular solution.
	\item[5)] If no breaking condition is satisfied, return to step 3.
\end{itemize}

Thus, we can see that the algorithm is very simple and clear, something that will be helpful for developing it as a function in R in section c. 

Based on the previous discussion of the main steps of the algorithms, we can explicitly develop the pseudo-code of it as follows:

\begin{algorithm}
\caption{EM algorithm for upper censored regression}\label{A1}
\begin{algorithmic}[1]
	\Procedure{Step 0: Initialization}{}
	\State $MaxIters, \epsilon, n, \tau, Convergence, \theta_t, LL_t$ \hfill \# Global Parameters
	\EndProcedure
	\Procedure{Step 1: Data Generation/Importation}{}
	\If{No Data}
	\State $X_{Obs}, Y_{Obs}, X_{Cen}, Y_{Cen} \leftarrow GenerateData(n, \tau)$ \hfill \# Generate random data
	\EndIf 
	\EndProcedure
	\Procedure{Step 2: Initial starting point}{}
	\State $\theta_0 \leftarrow StartingPoint(X_{obs}, Y_{obs})$ \hfill \# Starting point using observed data.
	\EndProcedure
	\Procedure{Step 3: Main Loop}{}
	\State $t\leftarrow 1$ \hfill \# Current iteration
	\While{TRUE}
	\State $\beta_t \leftarrow ComputeBetas(X,Y, \tau, \theta_{t-1})$  \hfill \# New $\beta$ vector
	\State $\sigma_t \leftarrow ComputeSigma(X,Y, \beta_t)$ \hfill \# New $\sigma$ value
	\State $\theta_t \leftarrow (\beta_t, \sigma_t)$  	\hfill \# Update solutions
	\State $LL_t \leftarrow CalculateLL(\theta_t, \tau, X,Y)$  \hfill \# New objective value
	\If{$SolConverged(\epsilon, LL_t, LL_{t-1})$} \hfill \# Check if solution converged
	\State $\theta^{*} \leftarrow \theta_t$			\hfill \# Update the best solution
	\State $LL^{*} \leftarrow LL(\theta^{*}, X,Y,\tau)$
	\State $Convergence \leftarrow TRUE$ 
	\State \textbf{break}
	\EndIf
	\If{$t > MaxIters$}		\hfill \# Check max number of iterations
	\State \textbf{break}	
	\EndIf
	\EndWhile
	\EndProcedure
	\Procedure{Step 4: Return results}{}		
	\If{$Convergence == TRUE$}			\hfill \# If convergence is reached, report optimal
	\State \textbf{return}($\theta^{*}, Convergence)$
	\Else
	\State \textbf{return}($\theta_{t}, Convergence)$ \hfill \# Else, best solution
	\EndIf
	\EndProcedure
	\end{algorithmic}
\end{algorithm}

Therefore, we will implement this algorithm in R in section (c) of the current problem. 








\subsection*{b) Starting values: based on observations}
As seen in the previous section, the most reasonable values for starting the algorithm for $\beta_0$, $\beta_1$, and $\sigma$ are:

\begin{itemize}
	\item[i)] $(\beta_0, \beta_1)$: We can easily compute the coefficients associated with the linear regression over the observed values $y_{obs}, x_{obs}$. By a simple call of the $lm()$ function in R, we can estimate the $\hat{beta}$ coefficients and use them as natural starting points for our EM optimization algorithm.
	\item[ii)] $\sigma$: Following the logic of the previous section, we can initialize the variable of $\sigma$ by simply computing the standard deviation of the observed values $y_{obs}$ from the sample.
\end{itemize}

Therefore, we will use the following starting point:
\begin{eqnarray}
	\theta_0 &=& (\hat{\beta_0}, \hat{\beta_1}, \hat{\sigma}) \\
	&=& (0.1440021, 1.928731, 1.683601)
\end{eqnarray}



\subsection*{c) R function and tests}

Based on the pseudo-code and deep explanation of the algorithm included in the previous section, we will perform the following actions for implementing it:

\begin{itemize}
	\item[i)] A series of auxiliary functions will be created in order to easily perform iterative tasks while trying to minimize the source of error of the calculations along the same code.
	\item[ii)] Using these functions, a simple adaptation of the linear regression model function $lm()$ will be utilized in order to estimate the optimal $\beta$ vector. Then, these values will be used to calculate the optimal $\sigma$ value for the iteration.
	\item[iii)] Convergence will be checked as well as the number of iterations performed. If any breaking condition is satisfied, break, otherwise, we go to the next iteration. 
	\item[iv)] Following the statement of the problem, the data will be generated based on the $ps8.R$ file, testing two main instances with $20\%$ and $80\%$ of censored observations (associated with $\tau = 4$ and $\tau = 0$ after some preliminary tests with the code).
\end{itemize}

Thus, we developed the following code:
\begin{itemize}
  \item[1.] Based on the code provided in the $ps8.R$ file, we create an auxiliary function $GenData()$ for generating the random data from the normal distribution, including a threshold $\tau$ for determining whether the observation is censored or not. The generated data is separated by $(y,z)$ (observed and censored) in order to be able to perform all the following operations of the algorithm in a simple way.
\end{itemize}

```{r GenerateData, echo=TRUE, eval=TRUE, cache=TRUE}
GenData <- function(n=100, Tau=0){
  # Code from ps8.R file AS-IS
  set.seed(1)
  beta0 <- 1
  beta1 <- 2
  sigma2 <- 6
  
  x <- runif(n)
  yComplete <- rnorm(n, beta0 + beta1*x, sqrt(sigma2))
  
  ## parameters chose such that signal in data is moderately strong
  ## estimate divided by std error is ~ 3
  mod <- lm(yComplete ~ x)
  summary(mod)$coef
  # End of original code
  
  # Censored and observed values: separation
  yCen = yComplete[yComplete > Tau]
  yObs = yComplete[yComplete <= Tau]
  XCen = x[yComplete > Tau] 
  XObs = x[yComplete <= Tau]
  
  # Return all vectors
  return(list(yCen, yObs, XCen, XObs))
}

# Test the function
Data <- GenData(Tau = 4)
yCen <- Data[[1]]
yObs <- Data[[2]]
XCen <- Data[[3]]
XObs <- Data[[4]]

# Check lengths
print(length(yCen))
print(length(yObs))
```

\begin{itemize}
  \item[2.] An auxiliary $StartingP()$ function is defined in order to initialize the $theta_0$ vector following the previous discussion.
\end{itemize}

```{r InitialPoint, echo=TRUE, eval=TRUE, cache=TRUE}
# Calculate a good starting point based on the data
StartingP <- function(XObs, yObs){
  # Estimate betas and sigma from non-censored values
  Betas = unname(lm(yObs ~ XObs)$coefficients)
  Sigma = sd(yObs)
  unname(Betas, force = TRUE)
  
  return(c("Beta0" = Betas[1], 
           "Beta1" = Betas[2], 
           "Sigma" = Sigma))
}

# Test the function
theta0 <- StartingP(XObs, yObs)
print("Starting parameters:")
theta0
```

\begin{itemize}
  \item[3.] We define an auxiliary function for computing the $mu$ values. 
\end{itemize}

```{r muT, echo=TRUE, eval=TRUE, cache=TRUE}
# Compute the value of mu per iteration
muT <- function(X, thetas_t){
  # Return the mu value
  return(thetas_t["Beta0"] + thetas_t["Beta1"] * X)
}

# Test the function
Mu_T <- muT(XCen, theta0)
Mu_T
```

\begin{itemize}
  \item[4.] Following the same logic, we define an auxiliary function for calculating the $\tau^{*}$ values.
\end{itemize}

```{r TauT, echo=TRUE, eval=TRUE, cache=TRUE}
TauT <- function(theta_t, mu_t, Tau=0){
  # Return the standardized value
  return( (Tau - mu_t) / theta_t["Sigma"] )
}

# Test the function
Tau_T <- TauT(theta0, Mu_T, 4)
Tau_T
```

\begin{itemize}
  \item[5.] Same with $\rho(\tau^{*})$:
\end{itemize}

```{r rhoT, echo=TRUE, eval=TRUE, cache=TRUE}
rhoT <- function(Tau_t){
  # Calculate the pdf and cdf, return the specific ratio
  pdf = dnorm(x = Tau_t, mean = 0, sd = 1)
  cdf = pnorm(q = Tau_t, mean = 0, sd = 1)
  return(pdf / (1 - cdf))
}

# Test the function
Rho_T <- rhoT(Tau_T)
Rho_T
```

\begin{itemize}
  \item[6.] We define the $Yexp()$ function for calculating the expected value of the censored values, needed for optimizing the $Q$ function.
\end{itemize}

```{r yexpected, echo=TRUE, eval=TRUE, cache=TRUE}
Yexp <- function(mu_t, theta_t, rho_t){
  # Compute the expected value of the censored components
  yexp <- mu_t + theta_t["Sigma"] * rho_t 
  return(unname(yexp))
}

# Test the function
YExp_T <- Yexp(Mu_T, theta0, Rho_T)
YExp_T
```

\begin{itemize}
  \item[7.] A very simple wrapper to the $lm()$ function is defined in order to perform the linear regression with the observed and conditional expected values of the censored components of the data in order to estimate the best $\beta$ vector for the iteration $t$.
\end{itemize}

```{r lmWrap, echo=TRUE, eval=TRUE, cache=TRUE}
lmWrap <- function(Xobs, Xcen, yobs, yexp){
  # Create the full vectors
  X <- c(Xobs, Xcen)
  Y <- c(yobs, yexp)
  
  # Calculate the regression model
  model <- lm(Y ~ X)
  
  # Coefficents
  BetasNew <- model$coefficients
  names(BetasNew) <- c("Beta0", "Beta1")
  
  # Return coefficients
  return(BetasNew)
}

# Test the function
Betas_New <- lmWrap(XObs, XCen, yObs, YExp_T)
Betas_New
```

\begin{itemize}
  \item[8.] Direct calculation of the square difference term (including observed and censored components) is performed via the function $ATerm()$.
\end{itemize}

```{r ATerm, echo=TRUE, eval=TRUE, cache=TRUE}
ATerm <- function(xobs, xcen, yobs, mu_t, rho_t, BetasNew, thetas_t){
  # Calculate the observed and censored terms of the expression
  Obs <- (yobs - BetasNew["Beta0"] - BetasNew["Beta1"] * xobs) ^ 2
  Cen <- (Yexp(mu_t, thetas_t, rho_t) - BetasNew["Beta0"] - BetasNew["Beta1"] * xcen) ^ 2
  
  # Return total summation
  return(sum(Obs) + sum(Cen))
}

# Test the function
AVal_T <- ATerm(XObs, XCen, yObs, Mu_T, Rho_T, Betas_New, theta0)
AVal_T
```

\begin{itemize}
  \item[9.] Similarly, the summation of the variances of the censored components is performed by calling the function $BTerm()$.
\end{itemize}

```{r BTerm, echo=TRUE, eval=TRUE, cache=TRUE}
BTerm <- function(theta_t, rho_t, tau_t){
  # Calculate the expression
  B <- (theta_t["Sigma"] ^ 2) * sum(1 + tau_t * rho_t - rho_t ^ 2)
  
  # Return total summation
  return(unname(B))
}

# Test the function
BVal_T <- BTerm(theta0, Rho_T, Tau_T)
BVal_T
```

\begin{itemize}
  \item[10.] The $SigmaOpt()$ function will find the optimal $\sigma$ value for the iteration based on the expressions that we found in the previous sections.
\end{itemize}


```{r SigmaNew, echo=TRUE, eval=TRUE, cache=TRUE}
SigmaOpt <- function(n, A, B){
  # Return the optimal value (from derivative)
  return(sqrt((A + B) / n ))  
}

# Test the function
n <- 100
SigmaNew <- SigmaOpt(n, AVal_T, BVal_T)
SigmaNew
```

\begin{itemize}
  \item[11.] Finally, we define the $LLval()$ function for keeping track of the current objective value.
\end{itemize}

```{r LL, echo=TRUE, eval=TRUE, cache=TRUE}
LLval <- function(A, B, n, ThetasNew){
  # Calculate the expression by pieces
  Exp1 <- - n * log(ThetasNew["Sigma"])
  Exp2 <- - (1 / (2 * (ThetasNew["Sigma"] ^ 2))) * (A + B)
  
  # Return final summation
  return(unname(Exp1 + Exp2))
}

# Test the function
ThetasNew <- c(Betas_New, "Sigma" = SigmaNew)
LLVal_T <- LLval(AVal_T, BVal_T, n, ThetasNew)
ThetasNew
LLVal_T
```


\begin{itemize}
  \item[12.] Based on all the previous analysis and defined functions, we have the main EM function. It follows the algorithm already discussed and presented as pseudo-code in the previous sections: (1) Data is generated, (2) Global parameters are defined, and a (3) Reasonable starting point is initialized. 
\end{itemize}

```{r Alg, echo=TRUE, eval=FALSE, cache=TRUE}
EMOpt <- function(Tau, n=100, MaxIters=1e2, Epsilon=1e-3){  
  # Generate Data
  Data <- GenData(n, Tau)
  yCen <- Data[[1]]
  yObs <- Data[[2]]
  XCen <- Data[[3]]
  XObs <- Data[[4]]
  
  # Check % of censored
  print(paste0("Number of censored observations (out of ", n, "): ", length(yCen)))
  
  # Global Parameters 
  # List containing the Theta Vectors and vector with LL values
  Thetas_T <- rep(list(c(0,0,0)), MaxIters)
  LL_T <- rep(0, MaxIters)
  
  # Initial Theta values: Call the starting point generation function
  theta0 <- StartingP(XObs, yObs)
  
  # Auxiliary counter for number of iteration
  niter <- 1
  
  # Convergence flag (True is converged)
  Convergence <- FALSE
  
  # Pre-loop
  print("Starting point for the EM algorithm, Theta_0:")
  print(theta0)
  
  # Record initial values inside the list
  Thetas_T[[niter]] <- theta0
```

\begin{itemize}
  \item[] The main loop of the algorithm: all elements needed are calculated iteratively in order to estimate the optimal $\beta$ and $\sigma$ parameters for the current iteration. Values are recorded.
\end{itemize}

```{r Alg2, echo=TRUE, eval=FALSE, cache=TRUE}
  # Main loop (while true until break condition(s) reached)
  while(0==0){
    ## Iteration info
    print(paste0("---------------- Iteration ", niter, " ----------------"))
    print(paste0("Theta_", niter - 1, ":"))
    print(Thetas_T[[niter]])
    
    ## Step 1: Calculate the Optimal Betas and Sigma 
    # Betas: Compute all the expressions and solve an adapted linear regression model
    Mu_T <- muT(XCen, Thetas_T[[niter]])
    Tau_T <- TauT(Thetas_T[[niter]], Mu_T, Tau)
    Rho_T <- rhoT(Tau_T)
    YExp_T <- Yexp(Mu_T, Thetas_T[[niter]], Rho_T)
    
    Betas_New <- lmWrap(XObs, XCen, yObs, YExp_T)
    
    # Sigma: Compute all the expressions using Betas_T
    AVal_T <- ATerm(XObs, XCen, yObs, Mu_T, Rho_T, Betas_New, Thetas_T[[niter]])
    BVal_T <- BTerm(Thetas_T[[niter]], Rho_T, Tau_T)
    Sigma_New <- SigmaOpt(n, AVal_T, BVal_T)
    
    # Record new values
    NewVals = c(Betas_New["Beta0"], 
                Betas_New["Beta1"], 
                "Sigma" = Sigma_New)
    
    # Display new values:
    print(paste0("Optimal Theta_", niter," values obtained:"))
    print(NewVals)
    
```

\begin{itemize}
  \item[] The objective function is calculated and the vector containing all the solutions per iteration is updated.
\end{itemize}

```{r Alg3, echo=TRUE, eval=FALSE, cache=TRUE}
    ## Step 2: Compute the LL function value
    LL_T[niter] <- LLval(AVal_T, BVal_T, n, NewVals)
    print("Log-likelihood value:")
    print(LL_T[niter])
    
    ## Step 3: Update Results
    Thetas_T[[niter + 1]] <- NewVals
    
```

\begin{itemize}
  \item[] Break conditions are tested: Convergence and a maximum number of iterations. If none of them is $TRUE$, go to the next iteration and repeat the procedure.
\end{itemize}


```{r Alg4, echo=TRUE, eval=FALSE, cache=TRUE}
    ## Step 4: Check breaking conditions
    # Change of the objective function
    if (niter > 1){
      if (abs(LL_T[niter - 1] - LL_T[niter]) <= Epsilon) {
        print("Convergence of the objective function reached, break")
        Convergence <- TRUE
        break
      }  
    }
    
    # Maximum number of iterations
    if (niter >= MaxIters){
      print("Maximum number of iterations reached, break")
      break
    }
    
    # If no break, next iteration
    niter <- niter + 1
    
  }
```

\begin{itemize}
  \item[] After finishing the algorithmic steps, a final summary table is printed out to the console in order to allow the user to check the evolution of the algorithm as well as check all the relevant information of the EM approach. The DataFrame is returned to the user.
\end{itemize}

```{r Alg5, echo=TRUE, eval=FALSE, cache=TRUE}
  # Final Summary
  SummaryDF <- data.frame("Iteration" = seq(1, niter, 1),
                          "Beta0" = unlist(Thetas_T[1:niter])[seq(1, (niter) * 3, 3)],
                          "Beta1" = unlist(Thetas_T[1:niter])[seq(2, (niter) * 3, 3)],
                          "Sigma" = unlist(Thetas_T[1:niter])[seq(3, (niter) * 3, 3)],
                          "LL_Val" = LL_T[1:niter])
  
  print("---------- Summary Results ----------")
  SummaryDF
  print(paste0("Convergence Status: ", Convergence))
  print(paste0("Number of iterations: ", niter ))
  
  # Return solution
  return(SummaryDF)
}
```


Therefore, we are ready to test the function using $\tau = 4$ (20\% of censored data) and $\tau = 0$ (80\% of censored data). \textbf{Note:} We added a $verbose$ flag to the previous code in order to not print-out all the information per iteration on the console for visualization purposes, just showing the final summary DataFrame in the following tests. 

```{r fullEM, echo=FALSE, eval=TRUE, cache=TRUE}
EMOpt <- function(Tau, n=100, MaxIters=1e2, Epsilon=1e-3, verbose=FALSE){  
  # Generate Data
  Data <- GenData(n, Tau)
  yCen <- Data[[1]]
  yObs <- Data[[2]]
  XCen <- Data[[3]]
  XObs <- Data[[4]]
  
  # Check % of censored
  if (verbose == TRUE){
    print(paste0("Number of censored observations (out of ", n, "): ", length(yCen)))
  }
  
  # Global Parameters 
  # List containing the Theta Vectors and vector with LL values
  Thetas_T <- rep(list(c(0,0,0)), MaxIters)
  LL_T <- rep(0, MaxIters)
  
  # Initial Theta values: Call the starting point generation function
  theta0 <- StartingP(XObs, yObs)
  
  # Auxiliary counter for number of iteration
  niter <- 1
  
  # Convergence flag (True is converged)
  Convergence <- FALSE
  
  # Pre-loop
  if (verbose == TRUE){
    print("Starting point for the EM algorithm, Theta_0:")
    print(theta0)
  }
  
  # Record initial values inside the list
  Thetas_T[[niter]] <- theta0

  # Main loop (while true until break condition(s) reached)
  while(0==0){
    ## Iteration info
    if (verbose == TRUE){
      print(paste0("---------------- Iteration ", niter, " ----------------"))
      print(paste0("Theta_", niter - 1, ":"))
      print(Thetas_T[[niter]])
    }
    
    ## Step 1: Calculate the Optimal Betas and Sigma 
    # Betas: Compute all the expressions and solve an adapted linear regression model
    Mu_T <- muT(XCen, Thetas_T[[niter]])
    Tau_T <- TauT(Thetas_T[[niter]], Mu_T, Tau)
    Rho_T <- rhoT(Tau_T)
    YExp_T <- Yexp(Mu_T, Thetas_T[[niter]], Rho_T)
    
    Betas_New <- lmWrap(XObs, XCen, yObs, YExp_T)
    
    # Sigma: Compute all the expressions using Betas_T
    AVal_T <- ATerm(XObs, XCen, yObs, Mu_T, Rho_T, Betas_New, Thetas_T[[niter]])
    BVal_T <- BTerm(Thetas_T[[niter]], Rho_T, Tau_T)
    Sigma_New <- SigmaOpt(n, AVal_T, BVal_T)
    
    # Record new values
    NewVals = c(Betas_New["Beta0"], 
                Betas_New["Beta1"], 
                "Sigma" = Sigma_New)
    
    # Display new values:
    if (verbose == TRUE){
      print(paste0("Optimal Theta_", niter," values obtained:"))
      print(NewVals)
    }

    ## Step 2: Compute the LL function value
    LL_T[niter] <- LLval(AVal_T, BVal_T, n, NewVals)
    if (verbose == TRUE){
      print("Log-likelihood value:")
      print(LL_T[niter])
    }
    
    ## Step 3: Update Results
    Thetas_T[[niter + 1]] <- NewVals
    
    ## Step 4: Check breaking conditions
    # Change of the objective function
    if (niter > 1){
      if (abs(LL_T[niter - 1] - LL_T[niter]) <= Epsilon) {
        if (verbose == TRUE){
          print("Convergence of the objective function reached, break")
        }
        Convergence <- TRUE
        break
      }  
    }
    
    # Maximum number of iterations
    if (niter >= MaxIters){
      if (verbose == TRUE){
        print("Maximum number of iterations reached, break")
      }
      break
    }
    
    # If no break, next iteration
    niter <- niter + 1
    
  }

  # Final Summary
  SummaryDF <- data.frame("Iteration" = seq(1, niter, 1),
                          "Beta0" = unlist(Thetas_T[1:niter])[seq(1, (niter) * 3, 3)],
                          "Beta1" = unlist(Thetas_T[1:niter])[seq(2, (niter) * 3, 3)],
                          "Sigma" = unlist(Thetas_T[1:niter])[seq(3, (niter) * 3, 3)],
                          "LL_Val" = LL_T[1:niter])
  
  print("---------- Summary Results ----------")
  SummaryDF
  print(paste0("Convergence Status: ", Convergence))
  print(paste0("Number of iterations: ", niter ))
  
  # Return solution
  return(SummaryDF)
}
```

```{r Test1, echo=TRUE, eval=TRUE, cache=TRUE}
Result1 <- EMOpt(Tau=4, n=100, MaxIters=1e2, Epsilon=1e-3, verbose=FALSE)
print(Result1)
```

Based on the results we can see that the algorithm converges after $10$ iterations (very fast), obtaining an optimal vector $\theta_{10}$. Now, we solve the optimization problem for the instance with 80\% of censored values ($\tau = 0$):

```{r Test2, echo=TRUE, eval=TRUE, cache=TRUE}
Result2 <- EMOpt(Tau=0, n=100, MaxIters=2e2, Epsilon=1e-3, verbose=FALSE)
print(Result2)
```

Looking at the results, we can see in this case, that the algorithm did not achieved the desired convergence within 100 iterations, iterating until it reaches the maximum number of iterations condition and thus, we can conclude that it is not suitable for this particular case. In order to reach the convergence, we needed to increase the maximum number of iterations up to 102. As expected, the convergence of the algorithm is very slow since we are dealing with a very high percentage of non-observable/censored data.

\newpage




\subsection{d) Direct optimization: optim() and BFGS}

In this case, we will solve the same test instances by a direct optimization of the log-likelihood function without needing to perform an iterative EM approach. In order to perform these operations, we will define the observed log-likelihood function as follows:
\begin{eqnarray}
  L(\theta / y) &=& \prod_{i = 1}^{m} f(y_i) \prod_{j = m+1}^{n} [1 - F(\tau)] \\
  &=& \prod_{i=1}^{m} \phi \left( \dfrac{y_i - x_{i}^{'} \beta}{\sigma}  \right) \sigma^{-1} \prod_{j = m+1}^{n} [1- F(\tau)] \\
  &=& \prod_{i=1}^{m} \phi \left( \dfrac{y_i - x_{i}^{'} \beta}{\sigma}  \right) \sigma^{-1} \prod_{j = m+1}^{n} [1- \Phi \left( \dfrac{\tau-x_{j}^{'}\beta}{\sigma} \right)]
\end{eqnarray}

From this previous expression, we can easily compute the observed log-likelihood by applying the $log()$ function:
\begin{eqnarray}
  \log(L(\theta / y)) &=& \sum_{i=1}^{m} \log \left[ \phi \left( \dfrac{y_i - x_{i}^{'} \beta}{\sigma}  \right) \sigma^{-1} \right] + \sum_{j = m+1}^{n} \log \left( 1- \Phi \left( \dfrac{\tau-x_{j}^{'}\beta}{\sigma} \right) \right) \\
  &=& - m \log(\sigma) - \dfrac{1}{2 \sigma^{2}} \sum_{i=1}^{m} {(y_i - x_{i}^{'} \beta)}^{2} + \sum_{j = m+1}^{n} \log \left( 1- \Phi \left( \dfrac{\tau-x_{j}^{'}\beta}{\sigma} \right) \right)
\end{eqnarray}

\begin{itemize}
  \item[1.] Based on the previous analysis, we can define the $LLObserved\_Optim\_SigmaLog()$ function, taking into account the relevant issue indicated in Piazza by the instructor regarding the treatment of the $\sigma$ variable using the $log()$ function and then recalculating it inside the function for avoiding problems and NaN issues within the optimization approach.
\end{itemize}

```{r DirectAlg, echo=TRUE, eval=TRUE, cache=TRUE}
LLObserved_Optim_SigmaLog <- function(par, Tau, yObs, XObs, yCen, XCen){
  # Observable term
  Obs1 <- - (1 / (2 * (exp(par[3]) ^ 2)))  
  Obs2 <- sum((yObs - par[1] - par[2] * XObs) ^ 2)
  Obs3 <- - length(yObs) * par[3]
  Obs <- Obs1 * Obs2 + Obs3
  
  # Standardized value
  cdfVal <- (Tau - par[1] - XCen * par[2]) / exp(par[3]) 
  
  # Censored term
  Cen <- sum(log(1 - pnorm(q = cdfVal, mean = 0, sd = 1)) )
   
  # Return the LL value
  return(Obs + Cen)
}
```

\begin{itemize}
  \item[2.] Therefore, we generate the first test instance with 20\% of censored variables and we optimize the function.
\end{itemize}


```{r DirectAlg2, echo=TRUE, eval=TRUE, cache=TRUE}
# Optimize the function
# Model Data
n <- 100
Tau <- 4

# Generate Data
Data <- GenData(n, Tau)
yCen <- Data[[1]]
yObs <- Data[[2]]
XCen <- Data[[3]]
XObs <- Data[[4]]

# Check % of censored
print(paste0("Number of censored observations (out of ", n, "): ", length(yCen)))

# Optimize the LL function by using optim()
Results = optim(par=c(0.1440021, 1.928731, 1.683601), LLObserved_Optim_SigmaLog, 
                Tau = Tau, yObs = yObs, XObs = XObs, 
                yCen=yCen, XCen = XCen, 
                method = "BFGS", control=list(fnscale=-1, trace=TRUE, REPORT=1))

# Obtain the optimal theta values
OptTheta <- Results$par
OptTheta[3] <- exp(OptTheta[3])
names(OptTheta) <- c("Beta0", "Beta1", "Sigma")

# Print summary
print(paste0("Optimal LL Value: ",Results$value))
print("Optimal Theta vector:")
print(OptTheta)
print("Number of iterations:")
print(Results$counts)
```

Based on the results, we can see that: (1) the number of iterations is larger (11 gradient calls and 32 function calls vs 10 iterations of the EM algorithm) than in the case of the EM algorithm, and (2) the results obtained are almost identical to the ones achieved by the EM algorithm. Therefore, we are able to conclude that our EM algorithm implementation has been successful and arises as a very useful alternative for maximizing the likelihood of problems with censored data. Now, we proceed to optimize the model with 80\% of censored data.

```{r DirectAlg3, echo=TRUE, eval=TRUE, cache=TRUE}
# Optimize the function
# Model Data
n <- 100
Tau <- 0

# Generate Data
Data <- GenData(n, Tau)
yCen <- Data[[1]]
yObs <- Data[[2]]
XCen <- Data[[3]]
XObs <- Data[[4]]

# Check % of censored
print(paste0("Number of censored observations (out of ", n, "): ", length(yCen)))

# Optimize the LL function by using optim()
Results = optim(par=c( -1.29215258, 0.3748579, 0.9847210), LLObserved_Optim_SigmaLog, 
                Tau = Tau, yObs = yObs, XObs = XObs, 
                yCen=yCen, XCen = XCen, 
                method = "BFGS", control=list(fnscale=-1, trace=TRUE, REPORT=1))

# Obtain the optimal theta values
OptTheta <- Results$par
OptTheta[3] <- exp(OptTheta[3])
names(OptTheta) <- c("Beta0", "Beta1", "Sigma")

# Print summary
print(paste0("Optimal LL Value: ",Results$value))
print("Optimal Theta vector:")
print(OptTheta)
print("Number of iterations:")
print(Results$counts)
```

In this case, the convergence is faster than in the EM algorithm requiring only 12 gradient iterations and 36 calls to the function, in comparison to the 102 iterations performed by the EM algorithm in order to reach the convergence tolerance threshold. Again, we can see that both solutions are almost identical, telling us that our EM algorithm implementation has been successful.

