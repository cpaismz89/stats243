•	What are the goals of their simulation study and what are the metrics that they consider in assessing their method? 

a) The main and specific goals of the simulation study are
i) Develop and demonstrate the usefulness and accuracy of a new (adjusted) statistical test for determining if a 
random sample is drawn from a n-component normal mixture distribution or, as the alternative hypothesis if the 
sample is drawn from a l-component normal mixture. 

This test is derived from a theorem proposed by Vuong that it is extended in the current article and uses the likelihood 
ratio statistic based on the known Kullback-Leibler information criterion. One of the most relevant characteristics of this 
implementation is that "the results do not require that either of the competing models be correctly specified’".

ii) Demonstrate that the asymptotic distribution of the used LR statistic follows a distribution composed by a weighted 
sum of chi-squared random variables (independent) with one-degree of freedom when some assumptions (regularity 
conditions) are satisfied.

b) The metrics that the authors consider in assessing their method are:
i) The authors are comparing the P-values obtained by both the unadjusted and adjusted models for different significance 
levels (95% and 99%) for each configuration implemented.

ii) A second metric used consists of the value of  "Simulated powers, in percentages’" in order to compare different configurations 
and significance levels tested, as seen in Table 2.

iii) "Simulated significance levels, in percentages’" as seen in Table 3 when comparing the results of the unadjusted and 
adjusted models for a mixture of two normals versus a mixture of three normals using 1000 replications for estimating the 
parameters. 



•	What choices did the authors have to make in designing their simulation study? What are the key aspects of the data 
generating mechanism that likely affect the statistical power of the test? Are there data-generating scenarios that the 
authors did not consider that would be useful to consider? 

1. Design
The choices that the authors made in order to design their simulation study were:

i) The algorithm used to estimate the maximum likelihood value associated with the parameters/configuration under 
study. They implemented the EM algorithm.

ii) The number of sets used for random starting values parameters. The selected size was 100 sets of random starting values.

iii) The truncation error threshold. Defined as less than 10e-7.

iv) mu and sigma values

v) Sample sizes: $n = 50$, $75$, $100$, $150$, $200$, $300$, $400$, $500$ and $1000$ were considered.

vi) The number of replications per sample. 1000 replications were performed per sample size.

vii) Alpha values (significance level) tested. In this study, the authors selected the classic 0.05 and 0.01 values.

viii) Values of D, the variable that measures the distance between the two components of the normal mixture distribution.
	
ix) Mixing proportion pi parameter.


2. Key aspects

As we will discuss in the last question of the current section, the key aspects of the data generating mechanism that likely 
affect the statistical power of the test are:

i) Sample size: As the authors indicate in the paper, the power of the test is worse (even poor) when the sample size 
n is less than 200 elements. In particular, this situation arises when the two components of the distribution are not well 
separated (when D = 1 or 2). In addition, a sample of at least $100$ is required in order to reach a reasonable power 
when the components are pretty separated (D = 3). 

Therefore, we can clearly see the key role of the sampling size, associated with the value of D. Different configurations are tested.

ii) The number of replications: Not explicitly mentioned in the article but the number of replications performed is clearly one 
of the most important elements when conducting a simulation study. The authors do not discuss how they decide the number 
of replications (1000), however, they are clearly taking into account possible bias and/or errors produced when the size of the 
sampling is not enough for obtaining a small confidence interval for the estimated parameters such that the “real” value that they
want to estimate is associated with a large error margin. More discussion about this phenomenon is found in the last question of this section.


iii) D parameter value: Depending on the distance between the components, the authors find that the statistical power of the test is 
significantly affected. Based on the results in Table 2 and 3, the authors indicate that the "approximation of the nominal sizes to the 
actual sizes of the unadjusted test is inadequate for D=2 or 3". 

Following the same logic, the article indicates that the rate of convergence obtained for the adjusted test toward the asymptotic 
distribution is affected by the amount of space between the components of the distribution and also, by the mixing proportion. 
However, besides the mixing proportion selected, we have that the distribution of the adjusted test experiences a very good 
convergence when D=1 or 2. On the other hand, when D=3 the authors found that the approximation is not very accurate but “acceptable”.

As a conclusion, the authors indicate that "The results indicate that the factors with most influence on power are the spacings D1 and D2’’.
	

3. Extra scenarios
An interesting scenario for testing purposes would be to model distributions with different standard deviations/sigma (heteroscedasticity)
because the paper covers only homoscedastic cases. Different convergence and limiting properties should be studied and contrasted in 
order to perform and implement the adjusted version of the test.

As the authors mention in the future work section, we think that an interesting setting would be to extend the current adjusted test to 
other distributions such as exponential, Poisson, gamma, etc.



•	Do their tables do a good job of presenting the simulation results and do you have any alternative suggestions for how to do this?
i) Table 1 is good for visualizing the results but it can be improved. Table 1 could have been ordered by significance and tests (unadjusted 
or not) such that the comparison between both tests can be easily seen by the reader, without needing to look at a second table below and 
then look again at the first sub-table on the top (and correct row) for being able to compare the values obtained.

Example (see it in the latex file):
\begin{table}[h!]
\centering
\caption{New Table}
\label{Ntable}
\begin{tabular}{@{}cllllllllll@{}}
\multicolumn{1}{l}{\textbf{}} & \textbf{}                          & \multicolumn{9}{c}{\textbf{Sample size}}                                                                                                                                                                                                                                                                                \\ \midrule
\textbf{Nominal level}        & \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{50}} & \multicolumn{1}{c}{\textbf{75}} & \multicolumn{1}{c}{\textbf{100}} & \multicolumn{1}{c}{\textbf{150}} & \multicolumn{1}{c}{\textbf{200}} & \multicolumn{1}{c}{\textbf{300}} & \multicolumn{1}{c}{\textbf{400}} & \multicolumn{1}{c}{\textbf{500}} & \multicolumn{1}{c}{\textbf{1000}} \\ \midrule
\multirow{2}{*}{0.01}         & A                                  & 0.009                           & 0.014                           & 0.013                            & 0.010                            & 0.012                            & 0.012                            & 0.015                            & 0.015                            & 0.019                             \\
                              & B                                  & 0.005                           & 0.009                           & 0.006                            & 0.008                            & 0.008                            & 0.009                            & 0.008                            & 0.010                            & 0.016                             \\ \midrule
\multirow{2}{*}{0.05}         & A                                  & 0.078                           & 0.076                           & 0.072                            & 0.075                            & 0.061                            & 0.070                            & 0.067                            & 0.069                            & 0.061                             \\
                              & B                                  & 0.057                           & 0.054                           & 0.049                            & 0.047                            & 0.045                            & 0.050                            & 0.052                            & 0.051                            & 0.053                             \\ \bottomrule
\end{tabular}
\end{table}


ii) In this case, the comparison between the non-adjusted and adjusted tests is very easy to perform since the results are located 
side-by-side (as we suggested in the previous table).  For visualization purposes (just a minor change), the mixing proportion value 
could be placed in the middle of the “Sample sizes” values, in the same row as 100 in this case. It would be more explicit to indicate 
that each batch of 3 sample sizes corresponds to a particular mixing proportion.
 
Important is to note that a reader who did not read all the previous content of the paper will not be able to understand the meaning 
of D = I, I in  {1,2,3} since there is no explanation/caption associated with the table that adds information about these elements.

iii) The third Table is also good for presenting the results but its first column can be easily eliminated since there is only one value 
inside of it (Mixing proportion equal to 0.7) and thus, that information can be taken out from the Table and add it inside the caption 
or table title, in order to have a better looking (and clean) Table with the relevant results, allowing the authors to add more decimals 
if they want or simply have a better looking Table.

On the other hand, we can suppose that some readers are not reading 100% of the contents but they are interested in the results. 
In that case, a better caption/explanation should be useful to be with the Table in order to be able to gather the maximum information 
from them. In this particular case, we notice that a reader may not easily understand that 2LR* corresponds to the adjusted test (there 
is no legend/caption indicating that so we have to guess based on the previous sections) and the meaning of D_i is not clear just looking 
at the Table. Therefore, it is not self-content.

As with Table 1 and 2, the numeric format (decimals separator) is not the best one in our opinion because the reader is used to 
dots/commas as separators instead of a middle position dot. However, it can be a matter of journal format and in that case, authors 
needed to follow that special format.

iv) Table 4 can be misleading due to the fact that two mixing proportions are being used but there is no column indicating the proportion 
values as before. In addition, too many results are included in one table, it may be good (depending on the journal and the expected 
audience of the article) to split the results into two separated tables with a small discussion in between them.


•	Interpret their tables on power (Tables 2 and 4) - do the results make sense in terms of how the power varies as a function of the 
data generating mechanism? 

Remembering that the power of a binary hypothesis test consists of the probability that the test is rejecting the null hypothesis when 
the alternative one is true, we are interested in how the power of the article’s test is affected by the data generating mechanism and 
configurations tested. In addition, a simple power analysis can be performed in order to determine the minimum sample size that is 
needed in order to be able to reasonably detect a certain effect of a given size.

Classic/General factors influencing the power of a test are: 
a)	The significance level (alpha) where we can usually increase the power level of the test by selecting a larger significance level 
(e.g. 0.05 instead of 0.01). This will increase the chances of rejecting the null hypothesis when it is false (reduces Type II error) 
but at the same time, it increases the risk of obtaining a "significant result” when the null hypothesis is not false (Type I error).

b)	Sample size: effects are difficult to measure in smaller samples. Usually, larger sample sizes will increase the power of the test.

Other specific factors will influence the power of the statistical test depending on its particular characteristics.

Thus, based on the results from Tables 2 and 4, and all the previous development of the article about the characteristics of the 
test including its mathematical expression (adjusted and not adjusted), we have that:

1.	Table 2: single normal versus a two-component normal mixture	
In this case, we can see that the power of the test is not the best (very low) when the size of the sample is small (n less than 200) 
and the distance between the distributions is not very significant in order to be able to clearly identify the underlying distribution. 
These results are consistent with the authors (and ours) expectations since based on the mathematical expression of the test we can 
clearly see that it will lose power depending on the data generating mechanisms. Here, having sample sizes less than 200 lead to a 
poor approximation of the test to the limiting distribution while values of D = 1 or 2 do not help the test to distinguish between 
distributions: therefore, these two elements drive the test to poor results.

On the other hand, we can see that increasing the value of D (up to 3) will increase the power of the test as expected since it is 
easier to distinguish the components of one distribution from another. Hence, we can see from the paper that when D = 3 the 
minimum sample size needed for obtaining reasonable results decreases up to n = 100. Thus, there exists a clear and expected 
relationship between the possible configurations of the test and its outcome.

One interesting result is obtained regarding the effect of the mixing proportion where the authors indicate that “there is no 
strong evidence that the power depends on the mixing proportion”,  a result that does not follow one of the studies indicated 
during the introduction of the paper where we have that “Goffinet et al. (1992) considered the case where the mixing proportions
of a mixture distribution are known a priori. Their results suggest that the asymptotic distribution of the likelihood ratio test 
statistic depends on the value of the mixing proportion, the dimensionality of the problem and whether or not the within 
component variance is known”.

Finally, the conclusions derived from the unadjusted test where the authors indicate that its power is “inflated” because the 
approximation of both nominal levels (0.01 and 0.05) is not accurate to the actual levels of the test were expected based on the 
mathematical expression of the test.

Hence, there exists a clear relationship between the data generating mechanism (and configurations) and the power of the test 
where, as expected, a larger size of the sample will increase the power of the test as well as the distance between the distributions 
(easier to identify them). 

2.	Table 4: two-component normal mixture versus a three-component normal mixture
As with the previous results, in this case, the authors obtain similar results in terms of the effects that both the sample size 
and distance between the distribution components have in the power of the test. Like before, we have that the most important 
parameter in terms of the impact on the power of the test is the value of D, the spacing between the parameters of the distributions. 

Again, small values of D1 and D2 require larger sample sizes (n at least 200) in order to reach a good power level (and convergence 
of the test) while values of D1 and D2 greater or equal to 3 will need a smaller sample size (at least 100) in order to perform well 
in terms of p-values.

In addition, similar results are obtained – quantitatively similar results – for the different weight sets tested, and thus, results are 
consistent with the ones obtained in the second table, following the same pattern where mixing proportions were not clearly significant 
in terms of their impact on the power of the test.

Therefore, we can conclude that the results presented in the paper are consistent with our (and the authors) expectations and are coherent 
to the statistical theory regarding the power of a test, based on the data generating mechanism and configuration employed.


•	How do you think the authors decided to use 1000 simulations? Would 10 simulations be enough? How might we decide if 
1000 simulations is enough?

When we perform simulations, it is very important to be aware of the fact that we are developing a sampling from a certain 
population/distribution and that we are not going to obtain the “real value” of a certain parameter. Thus, instead of obtaining 
a unique value, we usually obtain a confidence interval for the variable/parameter we are simulating with its corresponding significance 
level (usually 95% or 99%). Computing the confidence interval is associated with some assumptions regarding the underlying distribution 
of the samples where the most commons are the normal (sigma known) and t-student (unknown standard deviation) distributions. 
Depending on the sampling size, the level of uncertainty (width of the confidence interval) associated with the estimated value will be 
different and hence, the posterior analysis using the estimated values can include a larger error margin.

Therefore, we need to decide the number of replications taking into account that the total sample size will directly affect the confidence 
interval value and thus, our certainty that the values obtained are a “real” (at least as good as possible) representation of the original 
population/distribution.

In the particular setting of the paper, the authors could have decided the number of simulations (1000) following a similar logic to the 
one exposed above: since they are investigating the finite sample properties of the proposed test, they should be certain that the values 
obtained are good representations of the underlying distributions and thus, a large number of replications such as 1000 is selected in order 
to take into account that the random sampling (maybe using different random seeds) is associated with a small confidence interval for 
each sample size (n = 50, 75, …,  1000).

Clearly, assuming that we do not know anything from the distribution/population, 10 replications seems a very small amount of replications 
in order to fulfill the described requirements. The sample can be completely biased due to the lack of more replications, inducing to completely 
different (and wrong) results and conclusions to the whole study. There are high chances that with this small amount of replications, the 
estimated values are not good representatives of the underlying distribution leading to wide confidence intervals for the relevant parameters/variables 
estimated, with the corresponding error factor that the study will “carry on” for each step/action performed after this step propagating the error 
to the rest of the paper results. Therefore, we might decide the number of replications (1000 or other value) taking into account the previous analysis 
regarding the potential error associated with the estimation. A simple way consists of looking at the confidence intervals obtained for each estimated 
parameter/variable and try to reduce them as much as we want within a certain threshold in order to balance the trade-off between accuracy and 
computational time (or money) needed to perform all the replications. 